{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import LayerIntegratedGradients, IntegratedGradients\n",
    "from collections import OrderedDict\n",
    "from transformers import XLNetTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modified_xlnet import XLNetForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/scratch/kh8fb/2020summer_research/trained_models/xlnet_imdb/imdb-finetuned-xlnet-model_1.pth\"\n",
    "model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\")\n",
    "model_states = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "new_model_states = OrderedDict()\n",
    "for state in model_states:\n",
    "    correct_state = state[7:]\n",
    "    new_model_states[correct_state] = model_states[state]\n",
    "model.load_state_dict(new_model_states)\n",
    "model.eval()\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"might not find it to totally be like Pokémon without Ash.\"                                                                                               \n",
    "sentence2 = \"But this is definitely a Pokémon movie and way better than most of their animated movies.\"                                                                                      \n",
    "\n",
    "features = tokenizer([sentence, sentence2], return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "\n",
    "input_ids = features[\"input_ids\"] #.to(\"cuda:0\")\n",
    "token_type_ids = features[\"token_type_ids\"] #.to(\"cuda:0\")\n",
    "attention_mask = features[\"attention_mask\"] #.to(\"cuda:0\")\n",
    "baseline_ids = torch.zeros(input_ids.shape, dtype=torch.int64, device=\"cpu\") #, device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_forward_func(inputs, model, tok_type_ids, att_mask):\n",
    "    \"\"\"                                                                                                                                                                            \n",
    "    Passes forward the inputs and relevant keyword arguments.                                                                                                                      \n",
    "                                                                                                                                                                                   \n",
    "    Parameters                                                                                                                                                                     \n",
    "    ----------                                                                                                                                                                     \n",
    "    inputs: torch.tensor(1, num_ids), dtype=torch.int64                                                                                                                            \n",
    "        Encoded form of the input sentence.                                                                                                                                        \n",
    "    tok_type_ids: torch.tensor(1, num_ids), dtype=torch.int64                                                                                                                      \n",
    "        Tensor to specify token type for the model.                                                                                                                                \n",
    "        Because sentiment analysis uses only one input, this is just a tensor of zeros.                                                                                            \n",
    "    att_mask: torch.tensor(1, num_ids), dtype=torch.int64                                                                                                                          \n",
    "        Tensor to specify attention masking for the model.                                                                                                                         \n",
    "                                                                                                                                                                                   \n",
    "    Returns                                                                                                                                                                        \n",
    "    -------                                                                                                                                                                        \n",
    "    outputs: torch.tensor(1, 2), dtype=torch.float32                                                                                                                               \n",
    "        Output classifications for the model.                                                                                                                                      \n",
    "    \"\"\"\n",
    "    print(\"running Output\")\n",
    "    print(\"Inputs shape: \", inputs.shape)\n",
    "    print(\"token type shape:\", tok_type_ids.shape)\n",
    "    print(\"att mask shape: \", att_mask.shape)\n",
    "    outputs = model(input_ids=inputs, token_type_ids=tok_type_ids, attention_mask=att_mask)\n",
    "    print(outputs[0])\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size:  100\n",
      "input ids shape: torch.Size([20, 100])\n",
      "self.word_embedding(input_ids) shape:  torch.Size([20, 100, 768])\n",
      "word embedding type:  <class 'torch.nn.modules.sparse.Embedding'>\n",
      "('Output_h shape: ', torch.Size([20, 100, 768]))\n",
      "Output_g shape: None\n",
      "('Output_h shape: ', torch.Size([20, 100, 768]))\n",
      "Output_g shape: None\n",
      "('Output_h shape: ', torch.Size([20, 100, 768]))\n",
      "Output_g shape: None\n",
      "('Output_h shape: ', torch.Size([20, 100, 768]))\n",
      "Output_g shape: None\n",
      "('Output_h shape: ', torch.Size([20, 100, 768]))\n",
      "Output_g shape: None\n",
      "('Output_h shape: ', torch.Size([20, 100, 768]))\n",
      "Output_g shape: None\n",
      "('Output_h shape: ', torch.Size([20, 100, 768]))\n",
      "Output_g shape: None\n",
      "('Output_h shape: ', torch.Size([20, 100, 768]))\n",
      "Output_g shape: None\n",
      "('Output_h shape: ', torch.Size([20, 100, 768]))\n",
      "Output_g shape: None\n",
      "('Output_h shape: ', torch.Size([20, 100, 768]))\n",
      "Output_g shape: None\n",
      "('Output_h shape: ', torch.Size([20, 100, 768]))\n",
      "Output_g shape: None\n",
      "('Output_h shape: ', torch.Size([20, 100, 768]))\n",
      "Output_g shape: None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.5426, -0.4964],\n",
       "         [-0.7126, -0.0405],\n",
       "         [-0.0765, -0.9918],\n",
       "         [-0.7048, -0.1508],\n",
       "         [-0.4918, -0.5851],\n",
       "         [-0.4771, -0.3674],\n",
       "         [-0.3369, -0.5912],\n",
       "         [-0.3918, -0.7830],\n",
       "         [-0.5138, -0.3121],\n",
       "         [-0.6954, -0.2804],\n",
       "         [-1.0564, -0.1028],\n",
       "         [-0.3149, -0.5230],\n",
       "         [-0.7914, -0.3071],\n",
       "         [-0.2181, -0.4175],\n",
       "         [-0.4536, -0.3613],\n",
       "         [-0.4210, -0.7020],\n",
       "         [-0.4556, -0.2807],\n",
       "         [-0.0180, -1.0441],\n",
       "         [-0.8096, -0.0700],\n",
       "         [-0.6632, -0.2860],\n",
       "         [-0.1843, -0.7645],\n",
       "         [-0.2577, -0.3888],\n",
       "         [-0.6528, -0.2826],\n",
       "         [-0.1008, -0.8373],\n",
       "         [-1.0375,  0.0479],\n",
       "         [-0.3798, -0.6566],\n",
       "         [-0.3688, -0.3710],\n",
       "         [-0.6740, -0.5063],\n",
       "         [-0.3206, -0.5344],\n",
       "         [ 0.1459, -1.1453],\n",
       "         [-0.8059,  0.0320],\n",
       "         [-0.7673, -0.1948],\n",
       "         [-0.4214, -0.6159],\n",
       "         [-0.3603, -0.4546],\n",
       "         [-1.1840,  0.6857],\n",
       "         [-0.7079, -0.5080],\n",
       "         [-1.0788,  0.1997],\n",
       "         [-0.5386, -0.5892],\n",
       "         [-0.8238,  0.0714],\n",
       "         [ 0.5707, -1.6212],\n",
       "         [ 0.1336, -1.0892],\n",
       "         [-1.0557, -0.1077],\n",
       "         [-0.2192, -0.8919],\n",
       "         [ 0.3534, -1.1792],\n",
       "         [ 0.3694, -1.5682],\n",
       "         [-0.6763, -0.4261],\n",
       "         [-0.3899, -0.4980],\n",
       "         [ 0.0460, -0.5842],\n",
       "         [-1.1131,  0.3775],\n",
       "         [-0.7797, -0.2021],\n",
       "         [-0.6298, -0.4921],\n",
       "         [-0.6697, -0.2535],\n",
       "         [-0.9550,  0.0252],\n",
       "         [-0.8341, -0.3057],\n",
       "         [-0.1977, -0.7785],\n",
       "         [ 0.0507, -0.9988],\n",
       "         [-0.3619, -0.3127],\n",
       "         [-0.4101, -0.3175],\n",
       "         [-0.8234, -0.3597],\n",
       "         [-1.2999,  0.1547],\n",
       "         [-1.2017,  0.5231],\n",
       "         [-0.7853, -0.1290],\n",
       "         [-0.5791, -0.4276],\n",
       "         [ 0.4881, -1.7575],\n",
       "         [-1.1483,  0.3663],\n",
       "         [ 0.2254, -1.2833],\n",
       "         [-0.3994, -0.4801],\n",
       "         [ 0.2457, -1.2525],\n",
       "         [ 0.5202, -1.5547],\n",
       "         [-1.0374, -0.2986],\n",
       "         [-0.6616, -0.0226],\n",
       "         [-0.7169, -0.2741],\n",
       "         [ 0.2183, -0.9827],\n",
       "         [-0.2634, -0.5361],\n",
       "         [-0.1851, -0.8074],\n",
       "         [-0.7266, -0.4255],\n",
       "         [ 0.4792, -0.9376],\n",
       "         [-0.3724, -0.2044],\n",
       "         [-0.3289, -0.8636],\n",
       "         [-0.4250, -0.0186],\n",
       "         [-1.0749, -0.1130],\n",
       "         [-0.8574, -0.2048],\n",
       "         [-1.0452,  0.2834],\n",
       "         [-0.4139, -0.6986],\n",
       "         [-0.7517,  0.0282],\n",
       "         [-0.4146, -0.5029],\n",
       "         [-0.6270, -0.3994],\n",
       "         [-0.8471, -0.4792],\n",
       "         [ 0.2476, -1.0438],\n",
       "         [-0.4916, -0.3414],\n",
       "         [-0.4071, -0.5787],\n",
       "         [-0.9213,  0.1980],\n",
       "         [-0.6621, -0.3312],\n",
       "         [-0.9627,  0.3715],\n",
       "         [-0.8726,  0.0456],\n",
       "         [-0.1021, -0.7177],\n",
       "         [-0.2411, -0.6919],\n",
       "         [-0.0934, -0.9369],\n",
       "         [ 0.0322, -0.9228],\n",
       "         [-0.6804, -0.1613]], grad_fn=<AddmmBackward>),)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "rand_inputs = torch.tensor(np.random.randint(0, 3000, size=(100, 20)), dtype=torch.int64)\n",
    "model.forward(rand_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = model.to('cuda:0')\n",
    "layer_integrated = LayerIntegratedGradients(sequence_forward_func, model.transformer.word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "captum inputs shape\n",
      "torch.Size([2, 20])\n",
      "torch.Size([2, 20])\n",
      "torch.Size([2, 20])\n",
      "forward_func args:  OrderedDict([('inputs', <Parameter \"inputs\">), ('model', <Parameter \"model\">), ('tok_type_ids', <Parameter \"tok_type_ids\">), ('att_mask', <Parameter \"att_mask\">)])\n",
      "running Output\n",
      "Inputs shape:  torch.Size([2, 20])\n",
      "token type shape: torch.Size([2, 20])\n",
      "att mask shape:  torch.Size([2, 20])\n",
      "Batch size:  2\n",
      "input ids shape: torch.Size([20, 2])\n",
      "EVAL TENSORS\n",
      "1\n",
      "self.word_embedding(input_ids) shape:  torch.Size([20, 2, 768])\n",
      "word embedding type:  <class 'torch.nn.modules.sparse.Embedding'>\n",
      "EVAL TENSORS\n",
      "1\n",
      "('Output_h shape: ', torch.Size([20, 2, 768]))\n",
      "Output_g shape: None\n",
      "('Output_h shape: ', torch.Size([20, 2, 768]))\n",
      "Output_g shape: None\n",
      "('Output_h shape: ', torch.Size([20, 2, 768]))\n",
      "Output_g shape: None\n",
      "('Output_h shape: ', torch.Size([20, 2, 768]))\n",
      "Output_g shape: None\n",
      "('Output_h shape: ', torch.Size([20, 2, 768]))\n",
      "Output_g shape: None\n",
      "('Output_h shape: ', torch.Size([20, 2, 768]))\n",
      "Output_g shape: None\n",
      "('Output_h shape: ', torch.Size([20, 2, 768]))\n",
      "Output_g shape: None\n",
      "('Output_h shape: ', torch.Size([20, 2, 768]))\n",
      "Output_g shape: None\n",
      "('Output_h shape: ', torch.Size([20, 2, 768]))\n",
      "Output_g shape: None\n",
      "('Output_h shape: ', torch.Size([20, 2, 768]))\n",
      "Output_g shape: None\n",
      "('Output_h shape: ', torch.Size([20, 2, 768]))\n",
      "Output_g shape: None\n",
      "('Output_h shape: ', torch.Size([20, 2, 768]))\n",
      "Output_g shape: None\n",
      "tensor([[-0.7939, -0.4248],\n",
      "        [-1.2740,  0.5472]], grad_fn=<AddmmBackward>)\n",
      "captum inputs shape\n",
      "torch.Size([2, 20])\n",
      "torch.Size([2, 20])\n",
      "torch.Size([2, 20])\n",
      "forward_func args:  OrderedDict([('inputs', <Parameter \"inputs\">), ('model', <Parameter \"model\">), ('tok_type_ids', <Parameter \"tok_type_ids\">), ('att_mask', <Parameter \"att_mask\">)])\n",
      "running Output\n",
      "Inputs shape:  torch.Size([2, 20])\n",
      "token type shape: torch.Size([2, 20])\n",
      "att mask shape:  torch.Size([2, 20])\n",
      "Batch size:  2\n",
      "input ids shape: torch.Size([20, 2])\n",
      "EVAL TENSORS\n",
      "1\n",
      "self.word_embedding(input_ids) shape:  torch.Size([20, 2, 768])\n",
      "word embedding type:  <class 'torch.nn.modules.sparse.Embedding'>\n",
      "EVAL TENSORS\n",
      "1\n",
      "('Output_h shape: ', torch.Size([20, 2, 768]))\n",
      "Output_g shape: None\n",
      "('Output_h shape: ', torch.Size([20, 2, 768]))\n",
      "Output_g shape: None\n",
      "('Output_h shape: ', torch.Size([20, 2, 768]))\n",
      "Output_g shape: None\n",
      "('Output_h shape: ', torch.Size([20, 2, 768]))\n",
      "Output_g shape: None\n",
      "('Output_h shape: ', torch.Size([20, 2, 768]))\n",
      "Output_g shape: None\n",
      "('Output_h shape: ', torch.Size([20, 2, 768]))\n",
      "Output_g shape: None\n",
      "('Output_h shape: ', torch.Size([20, 2, 768]))\n",
      "Output_g shape: None\n",
      "('Output_h shape: ', torch.Size([20, 2, 768]))\n",
      "Output_g shape: None\n",
      "('Output_h shape: ', torch.Size([20, 2, 768]))\n",
      "Output_g shape: None\n",
      "('Output_h shape: ', torch.Size([20, 2, 768]))\n",
      "Output_g shape: None\n",
      "('Output_h shape: ', torch.Size([20, 2, 768]))\n",
      "Output_g shape: None\n",
      "('Output_h shape: ', torch.Size([20, 2, 768]))\n",
      "Output_g shape: None\n",
      "tensor([[-1.3383,  0.3704],\n",
      "        [-1.3383,  0.3704]], grad_fn=<AddmmBackward>)\n",
      "captum inputs shape\n",
      "()\n",
      "torch.Size([100, 20])\n",
      "torch.Size([100, 20])\n",
      "torch.Size([100, 20])\n",
      "forward_func args:  OrderedDict([('inputs', <Parameter \"inputs\">), ('model', <Parameter \"model\">), ('tok_type_ids', <Parameter \"tok_type_ids\">), ('att_mask', <Parameter \"att_mask\">)])\n",
      "running Output\n",
      "Inputs shape:  torch.Size([100, 20])\n",
      "token type shape: torch.Size([100, 20])\n",
      "att mask shape:  torch.Size([100, 20])\n",
      "Batch size:  100\n",
      "input ids shape: torch.Size([20, 100])\n",
      "self.word_embedding(input_ids) shape:  torch.Size([1000, 2, 768])\n",
      "word embedding type:  <class 'torch.nn.modules.sparse.Embedding'>\n",
      "('Output_h shape: ', torch.Size([1000, 2, 768]))\n",
      "Output_g shape: None\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size of dimension does not match previous size, operand 1, dim 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-134ef05ad869>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m attrs = layer_integrated.attribute(inputs=input_ids,\n\u001b[0m\u001b[1;32m      2\u001b[0m                                      \u001b[0mbaselines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbaseline_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                      \u001b[0madditional_forward_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                      \u001b[0mn_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                      \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/torchtext-scripts/lib/python3.8/site-packages/captum/attr/_core/layer/layer_integrated_gradients.py\u001b[0m in \u001b[0;36mattribute\u001b[0;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, internal_batch_size, return_convergence_delta, attribute_to_layer_input)\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0minps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         )\n\u001b[0;32m--> 352\u001b[0;31m         attributions = self.ig.attribute(\n\u001b[0m\u001b[1;32m    353\u001b[0m             \u001b[0minputs_layer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0mbaselines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbaselines_layer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/torchtext-scripts/lib/python3.8/site-packages/captum/attr/_core/integrated_gradients.py\u001b[0m in \u001b[0;36mattribute\u001b[0;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, internal_batch_size, return_convergence_delta)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;31m# grads: dim -> (bsz * #steps x inputs[0].shape[1:], ...)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         grads = _batched_operator(\n\u001b[0m\u001b[1;32m    279\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0mscaled_features_tpl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/torchtext-scripts/lib/python3.8/site-packages/captum/attr/_utils/batching.py\u001b[0m in \u001b[0;36m_batched_operator\u001b[0;34m(operator, inputs, additional_forward_args, target_ind, internal_batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mresults\u001b[0m \u001b[0mof\u001b[0m \u001b[0meach\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m     all_outputs = [\n\u001b[0m\u001b[1;32m    157\u001b[0m         operator(\n\u001b[1;32m    158\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/torchtext-scripts/lib/python3.8/site-packages/captum/attr/_utils/batching.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \"\"\"\n\u001b[1;32m    156\u001b[0m     all_outputs = [\n\u001b[0;32m--> 157\u001b[0;31m         operator(\n\u001b[0m\u001b[1;32m    158\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0madditional_forward_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madditional\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/torchtext-scripts/lib/python3.8/site-packages/captum/attr/_core/layer/layer_integrated_gradients.py\u001b[0m in \u001b[0;36mgradient_func\u001b[0;34m(forward_fn, inputs, target_ind, additional_forward_args)\u001b[0m\n\u001b[1;32m    331\u001b[0m                     \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_forward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_forward_hook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m                 output = _run_forward(\n\u001b[0m\u001b[1;32m    334\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madditional_forward_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m                 )\n",
      "\u001b[0;32m~/.conda/envs/torchtext-scripts/lib/python3.8/site-packages/captum/attr/_utils/common.py\u001b[0m in \u001b[0;36m_run_forward\u001b[0;34m(forward_func, inputs, target, additional_forward_args)\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[0madditional_forward_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_format_additional_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madditional_forward_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m     output = forward_func(\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0madditional_forward_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madditional_forward_args\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-ccec61c562fc>\u001b[0m in \u001b[0;36msequence_forward_func\u001b[0;34m(inputs, model, tok_type_ids, att_mask)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"token type shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtok_type_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"att mask shape: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matt_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtok_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0matt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/torchtext-scripts/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/sfs/lustre/bahamut/scratch/kh8fb/2020summer_research/trained_models/xlnet_imdb/modified_xlnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, mems, perm_mask, target_mapping, token_type_ids, input_mask, head_mask, inputs_embeds, use_cache, labels, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m   1218\u001b[0m             \u001b[0mheads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m         \"\"\"\n\u001b[0;32m-> 1220\u001b[0;31m         transformer_outputs = self.transformer(\n\u001b[0m\u001b[1;32m   1221\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/torchtext-scripts/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/sfs/lustre/bahamut/scratch/kh8fb/2020summer_research/trained_models/xlnet_imdb/modified_xlnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, mems, perm_mask, target_mapping, token_type_ids, input_mask, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    948\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"output_g shape: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_g\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_g\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"Output_g shape: None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m             outputs = layer_module(\n\u001b[0m\u001b[1;32m    951\u001b[0m                 \u001b[0moutput_h\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m                 \u001b[0moutput_g\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/torchtext-scripts/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/sfs/lustre/bahamut/scratch/kh8fb/2020summer_research/trained_models/xlnet_imdb/modified_xlnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, output_h, output_g, attn_mask_h, attn_mask_g, r, seg_mat, mems, target_mapping, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m     ):\n\u001b[0;32m--> 506\u001b[0;31m         outputs = self.rel_attn(\n\u001b[0m\u001b[1;32m    507\u001b[0m             \u001b[0moutput_h\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m             \u001b[0moutput_g\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/torchtext-scripts/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/sfs/lustre/bahamut/scratch/kh8fb/2020summer_research/trained_models/xlnet_imdb/modified_xlnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, h, g, attn_mask_h, attn_mask_g, r, seg_mat, mems, target_mapping, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0;31m#print(\"v_head_h shape\", v_head_h.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;31m#print(\"k_head_r shape: \", k_head_r.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m             attn_vec = self.rel_attn_core(\n\u001b[0m\u001b[1;32m    440\u001b[0m                 \u001b[0mq_head_h\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                 \u001b[0mk_head_h\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/sfs/lustre/bahamut/scratch/kh8fb/2020summer_research/trained_models/xlnet_imdb/modified_xlnet.py\u001b[0m in \u001b[0;36mrel_attn_core\u001b[0;34m(self, q_head, k_head_h, v_head_h, k_head_r, seg_mat, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;31m#print(\"rel_attn_core r_r_bias + q_head\", (q_head + self.r_r_bias).shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mbd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ibnd,jbnd->bnij\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_head\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr_r_bias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_head_r\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0mbd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrel_shift_bnij\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mklen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mac\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/torchtext-scripts/lib/python3.8/site-packages/torch/functional.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(equation, *operands)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;31m# the old interface of passing the operands as one list argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0moperands\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moperands\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size of dimension does not match previous size, operand 1, dim 1"
     ]
    }
   ],
   "source": [
    "attrs = layer_integrated.attribute(inputs=input_ids,\n",
    "                                     baselines=baseline_ids,\n",
    "                                     additional_forward_args=(model, token_type_ids, attention_mask),\n",
    "                                     n_steps=50,\n",
    "                                     target=0,\n",
    "                                     return_convergence_delta=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchtext-scripts3",
   "language": "python",
   "name": "torchtext-scripts3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
